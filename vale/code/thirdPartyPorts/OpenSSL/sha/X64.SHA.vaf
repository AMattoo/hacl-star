include "../../../arch/x64/X64.Vale.InsBasic.vaf"
//include "../../../arch/x64/X64.Vale.InsMem.vaf"
include "../../../arch/x64/X64.Vale.InsVector.vaf"
include "../../../arch/x64/X64.Vale.InsSha.vaf"

module X64.SHA

#verbatim{:interface}{:implementation}
open Opaque_s
open Types_s
open Words_s
open FStar.Seq
open X64.Machine_s
open X64.Memory
open X64.Vale.State
open X64.Vale.Decls
open X64.Vale.InsBasic
//open X64.Vale.InsMem
open X64.Vale.InsVector
open X64.Vale.InsSha
open X64.Vale.QuickCode
open X64.Vale.QuickCodes
open Arch.Types
open SHA_helpers
open Spec.SHA2Again
open GCM_helpers
open Spec.Loops
#endverbatim

#reset-options "--z3rlimit 20"

procedure {:quick} preamble(
        ghost ctx_b:buffer128
    )
    lets
        ctx @= rdi; tmp_reg @= rax;
        Wi @= xmm0; abef @= xmm1; cdgh @= xmm2; 
        tmp_xmm @= xmm7; bswap @= xmm8; 
    reads
        ctx; mem; memTaint;
    modifies
        tmp_reg;
        Wi; abef; cdgh; tmp_xmm; bswap; 
        efl;
    requires
        validSrcAddrs128(mem, ctx,  ctx_b,  2, memTaint, Secret);
    ensures
        // Why is this stored twice?
        tmp_xmm == bswap;
        bswap == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
        let abcd := buffer128_read(ctx_b, 0, mem) in
        let efgh := buffer128_read(ctx_b, 1, mem) in
        abef == Mkfour(efgh.lo1, efgh.lo0, abcd.lo1, abcd.lo0) /\ // LSB: FEBA
        cdgh == Mkfour(efgh.hi3, efgh.hi2, abcd.hi3, abcd.hi2);   // LSB: HGDC
        //make_hash(abef, cdgh) == // 0: HGFEDCBA
{
    // At the C level, ctx_b is an array of 8 32-bit values: ABCDEFGH
    // We load these out of memory using litte-endian loads, 
    // but SHA spec is big endian, so we need to do some swaps
    Load128_buffer(abef, ctx,  0, Secret, ctx_b, 0);  // abef := LSB: ABCD 
    Load128_buffer(cdgh, ctx, 16, Secret, ctx_b, 1);  // cdgh := LSB: EFGH
    InitPshufbMask(tmp_xmm, tmp_reg);

    Pshufd(Wi, abef, 0x1b);                           //   Wi := LSB: DCBA
    Pshufd(abef, abef, 0xb1);                         // abef := LSB: BADC
    Pshufd(cdgh, cdgh, 0x1b);                         // cdgh := LSB: HGFE
    
    Mov128(bswap, tmp_xmm);     // OpenSSL: offload   // bswap holds mask (why go via tmp?)
    Palignr8(abef, cdgh);                             // abef := LSB: FEBA == abef in big endian
    // OpenSSL uses punpcklqdq here:
    Shufpd(cdgh, Wi, 0);                              // cdgh := LSB: HGDC == cdgh in big endian
}

#reset-options "--z3rlimit 40"
procedure {:quick} loop_rounds_0_15(
        ghost ctx_b:buffer128,
        ghost in_b:buffer128,
        ghost k_b:buffer128
    )
    lets
        ctx @= rdi; inp @= rsi; tbl @= rcx;
        Wi @= xmm0; abef @= xmm1; cdgh @= xmm2; 
        msg0 @= xmm3; msg1 @= xmm4; msg2 @= xmm5; msg3 @= xmm6;
        tmp_xmm @= xmm7; abef_save @= xmm9; cdgh_save @= xmm10;
    reads
        ctx; tbl; mem; memTaint;
    modifies
        inp; 
        Wi; abef; cdgh; msg0; msg1; msg2; msg3; tmp_xmm; abef_save; cdgh_save;
        efl;
    requires
        validSrcAddrs128(mem, inp,   in_b,  4, memTaint, Secret);
        validSrcAddrs128(mem, ctx,  ctx_b,  2, memTaint, Secret);
        validSrcAddrs128(mem, tbl,    k_b, 16, memTaint, Secret);
        inp + 0x40 < pow2_64;

        // Why is this stored twice?
        tmp_xmm == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

        k_reqs(buffer128_as_seq(mem, k_b));
    ensures
        inp == old(inp) + 0x40;
        let input_LE := slice_work_around(buffer128_as_seq(mem, in_b), 4) in
        let input_BE := reverse_bytes_quad32_seq(input_LE) in
        let block:block_w(SHA2_256) := quads_to_block(input_BE) in
        // Wi == Don't care
        make_hash(abef, cdgh) == repeat_range_spec(0, 16, shuffle_core_opaque(SHA2_256, block), old(make_hash(abef, cdgh))) /\

        msg0 == ws_quad32(16, block) /\
        msg1 == add_wrap_quad32(ws_partial(20, block), ws_quad32(13, block)) /\
        msg2 == ws_partial(24, block) /\
        msg3 == ws_quad32(12, block);

        // tmp_xmm == Don't care
        abef_save == old(abef);
        cdgh_save == old(cdgh);
        
{
    // Create a ghost message block to pass to all of the sha256* instructions and lemmas
    ghost var input_LE := slice_work_around(buffer128_as_seq(mem, in_b), 4);
    ghost var input_BE := reverse_bytes_quad32_seq(input_LE);
    ghost var block:block_w(SHA2_256) := quads_to_block(input_BE);
    lemma_quads_to_block(input_BE);

    // Prove that the original hash input starts the repeat_range_spec base case
    ghost var hash_orig := make_hash(abef, cdgh);
    lemma_repeat_range_0(0, 0, shuffle_core_opaque(SHA2_256, block), hash_orig);

    ghost var ks := buffer128_as_seq(mem, k_b);

    // Load 4 128-bit message chunks == 16 32-bit chunks
    Load128_buffer(msg0, inp,  0, Secret, in_b, 0);
    Load128_buffer(msg1, inp, 16, Secret, in_b, 1);
    Load128_buffer(msg2, inp, 32, Secret, in_b, 2);
    Pshufb(msg0, tmp_xmm);       // Convert msg0 to big endian
    assert msg0 == index_work_around_quad32(input_BE, 0);   // OBSERVE: TODO: Better trigger for lemma_quads_to_block?
    Load128_buffer(msg3, inp, 48, Secret, in_b, 3);
    
    Load128_buffer(Wi, tbl, 0, Secret, k_b, 0);  // Load K values from memory
    assert Wi == index_work_around_quad32(ks, 0);
    Paddd(Wi, msg0);            // Combine the round constant with the message block

    Pshufb(msg1, tmp_xmm);      // Convert msg1 to big endian
    assert msg1 == index_work_around_quad32(input_BE, 1);   // OBSERVE: TODO: Better trigger for lemma_quads_to_block?
    Mov128(cdgh_save, cdgh);    // Save a copy of the original hash value to add at the end 

    // Do two rounds of SHA, drawing on state in ABEF, CDGH, and WK in lower64 of XMM0
    SHA256_rnds2(cdgh, abef, 0, block, hash_orig);
    Pshufd(Wi, Wi, 0x0e);       // Move upper-64 into lower-64 of Wi, lining up the next two WK values
    Mov128(abef_save, abef);    // Save a copy of the original hash value to add at the end
    SHA256_rnds2(abef, cdgh, 2, block, hash_orig);

    // Repeat above, except we also do some message expansion
    Load128_buffer(Wi, tbl, 16, Secret, k_b, 1);  // Load K values from memory
    assert Wi == index_work_around_quad32(ks, 1);
    Paddd(Wi, msg1);
    Pshufb(msg2, tmp_xmm);
    assert msg2 == index_work_around_quad32(input_BE, 2);   // OBSERVE: TODO: Better trigger for lemma_quads_to_block?
    SHA256_rnds2(cdgh, abef, 4, block, hash_orig);
    Pshufd(Wi, Wi, 0x0e);
    Add64(inp, 0x40); 
    SHA256_msg1(msg0, msg1, 16, block);     // Do a partial step of message expansion
    SHA256_rnds2(abef, cdgh, 6, block, hash_orig);

    // Repeat again
    Load128_buffer(Wi, tbl, 32, Secret, k_b, 2);  // Load K values from memory
    assert Wi == index_work_around_quad32(ks, 2);
    Paddd(Wi, msg2);
    Pshufb(msg3, tmp_xmm);
    assert msg3 == index_work_around_quad32(input_BE, 3);   // OBSERVE: TODO: Better trigger for lemma_quads_to_block?
    SHA256_rnds2(cdgh, abef, 8, block, hash_orig);
    Pshufd(Wi, Wi, 0x0e);
    Mov128(tmp_xmm, msg3);                  // We don't need the mask any more, but why save msg3?
    Palignr4(tmp_xmm, msg2);
    Paddd(msg0, tmp_xmm);
    SHA256_msg1(msg1, msg2, 20, block);     // Do another partial step of message expansion
    SHA256_rnds2(abef, cdgh, 10, block, hash_orig);

    // Repeat a final time
    Load128_buffer(Wi, tbl, 48, Secret, k_b, 3);  // Load K values from memory
    assert Wi == index_work_around_quad32(ks, 3);
    Paddd(Wi, msg3);
    SHA256_msg2(msg0, msg3, 16, block);      // Finalize the message expansion
    SHA256_rnds2(cdgh, abef, 12, block, hash_orig);
    Pshufd(Wi, Wi, 0x0e);
    Mov128(tmp_xmm, msg0);                  // Why save msg0?
    Palignr4(tmp_xmm, msg3);
    Paddd(msg1, tmp_xmm);
    SHA256_msg1(msg2, msg3, 24, block);     // Do another partial step of message expansion
    SHA256_rnds2(abef, cdgh, 14, block, hash_orig);
}

procedure {:quick} loop_rounds_16_51_body(
        inline i:nat,
        ghost k_b:buffer128,
        ghost block:block_w(SHA2_256),
        ghost hash_orig:hash_w(SHA2_256)
    )
    lets
        tbl @= rcx;
        Wi @= xmm0; abef @= xmm1; cdgh @= xmm2; 
        msg0 @= xmm3; msg1 @= xmm4; msg2 @= xmm5; msg3 @= xmm6;
        tmp_xmm @= xmm7;
    reads
        tbl; mem; memTaint;
    modifies
        Wi; abef; cdgh; msg0; msg1; msg2; msg3; tmp_xmm; 
        efl;
    requires
        validSrcAddrs128(mem, tbl, k_b, 16, memTaint, Secret);

        4 <= i /\ i < 13;

        k_reqs(buffer128_as_seq(mem, k_b));

        make_hash(abef, cdgh) == repeat_range_spec(0, 4*i, shuffle_core_opaque(SHA2_256, block), hash_orig);

        msg0 == ws_quad32(4*i, block) /\
        msg1 == add_wrap_quad32(ws_partial(4*(i+1), block), ws_quad32(4*(i-1)+1, block)) /\
        msg2 == ws_partial(4*(i+2), block) /\
        msg3 == ws_quad32(4*(i-1), block);

    ensures
        make_hash(abef, cdgh) == repeat_range_spec(0, 4*(i+1), shuffle_core_opaque(SHA2_256, block), hash_orig);

        msg1 == ws_quad32(4*(i+1), block);
        msg2 == add_wrap_quad32(ws_partial(4*(i+2), block), ws_quad32(4*(i)+1, block));
        msg3 == ws_partial(4*(i+3), block);
        msg0 == ws_quad32(4*(i), block);
{
    ghost var ks := buffer128_as_seq(mem, k_b);
    Load128_buffer(Wi, tbl, 16*i, Secret, k_b, i);  // Load K values from memory
    assert Wi == index_work_around_quad32(ks, i);
    
    Paddd(Wi, msg0);
    SHA256_msg2(msg1, msg0, 4*(i+1), block);
    SHA256_rnds2(cdgh, abef, 4*i, block, hash_orig);
    Pshufd(Wi, Wi, 0x0e);
    Mov128(tmp_xmm, msg1);
    Palignr4(tmp_xmm, msg0);
    Paddd(msg2, tmp_xmm);
    SHA256_msg1(msg3, msg0, 4*(i + 3), block);
    SHA256_rnds2(abef, cdgh, 4*i + 2, block, hash_orig);
}

// TODO: OpenSSL does this without any moves by renaming variables.  
//       We can do the same once Vale quick-mode supports that,
//       or we can rewrite the procedure above in non-quick mode
procedure {:quick} msg_shift()
    lets
        msg0 @= xmm3; msg1 @= xmm4; msg2 @= xmm5; msg3 @= xmm6;
        Wi @= xmm0; tmp_xmm @= xmm7;
    modifies
        msg0; msg1; msg2; msg3; tmp_xmm; Wi;
    ensures
        msg0 == old(msg1);
        msg1 == old(msg2);
        msg2 == old(msg3);
        msg3 == old(msg0);
{
    Mov128(tmp_xmm, msg0);
    Mov128(Wi, msg3);
    Mov128(msg0, msg1);
    Mov128(msg1, msg2);
    Mov128(msg3, tmp_xmm);
    Mov128(msg2, Wi);
}



// TODO: Wrap the above in a recursive loop that calls loop_rounds_16_55_body, then msg_shift



procedure {:quick} {:verify true} loop_rounds_52_64(
        ghost k_b:buffer128,
        ghost block:block_w(SHA2_256),
        ghost hash_orig:hash_w(SHA2_256)
        )
    lets
        tbl @= rcx; num @= rdx;
        Wi @= xmm0; abef @= xmm1; cdgh @= xmm2; 
        msg0 @= xmm3; msg1 @= xmm4; msg2 @= xmm5; msg3 @= xmm6;
        tmp_xmm @= xmm7; bswap @= xmm8; abef_save @= xmm9; cdgh_save @= xmm10;
    reads
        tbl; bswap; abef_save; cdgh_save; mem; memTaint;
    modifies
        num;
        Wi; abef; cdgh; msg0; msg1; msg2; msg3; tmp_xmm; 
        efl;
    requires
        validSrcAddrs128(mem, tbl, k_b, 16, memTaint, Secret);

        num > 0;

        k_reqs(buffer128_as_seq(mem, k_b));

        make_hash(abef, cdgh) == repeat_range_spec(0, 52, shuffle_core_opaque(SHA2_256, block), hash_orig);

        msg0 == ws_quad32(52, block); 
        msg1 == add_wrap_quad32(ws_partial(56, block), ws_quad32(49, block));
        msg2 == ws_partial(60, block);
        msg3 == ws_quad32(48, block);
    ensures
        tmp_xmm == old(bswap);
        num == old(num) - 1;
{
    ghost var ks := buffer128_as_seq(mem, k_b);
    Load128_buffer(Wi, tbl, 16*13, Secret, k_b, 13);  // Load K values from memory
    assert Wi == index_work_around_quad32(ks, 13);
    Paddd(Wi, msg0);
    SHA256_msg2(msg1, msg0, 4*(13+1), block);
    SHA256_rnds2(cdgh, abef, 4*13, block, hash_orig);
    Pshufd(Wi, Wi, 0x0e);
    Mov128(tmp_xmm, msg1);
    Palignr4(tmp_xmm, msg0);
    SHA256_rnds2(abef, cdgh, 4*13+2, block, hash_orig);
    Paddd(msg2, tmp_xmm);
    
    Load128_buffer(Wi, tbl, 16*14, Secret, k_b, 14);  // Load K values from memory
    assert Wi == index_work_around_quad32(ks, 14);
    Paddd(Wi, msg1);
    SHA256_rnds2(cdgh, abef, 4*14, block, hash_orig);
    Pshufd(Wi, Wi, 0x0e);
    SHA256_msg2(msg2, msg1, 4*(14+1), block);
    Mov128(tmp_xmm, bswap);
    SHA256_rnds2(abef, cdgh, 4*14+2, block, hash_orig);

    Load128_buffer(Wi, tbl, 16*15, Secret, k_b, 15);  // Load K values from memory
    assert Wi == index_work_around_quad32(ks, 15);
    Paddd(Wi, msg2);
    SHA256_rnds2(cdgh, abef, 4*15, block, hash_orig);
    Pshufd(Wi, Wi, 0x0e);
    Sub64(num, 1);
    SHA256_rnds2(abef, cdgh, 4*15+2, block, hash_orig);
    //assert make_hash(abef, cdgh) == shuffle(SHA2_256, hash_orig, block);

    Paddd(cdgh, cdgh_save);
    Paddd(abef, abef_save);
}
